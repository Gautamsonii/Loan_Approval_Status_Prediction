# -*- coding: utf-8 -*-
"""ML-Project-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10_lfivUG-1igAwVdmNeq9Kn7kVFY-46a
"""

# Importing the dependencies
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.metrics import accuracy_score

# Data Collection and Processing

# Loaded dataset to pandas dataframe
loan_ds = pd.read_csv('/content/sample_data/Loan_dataset.csv')

type(loan_ds)

loan_ds.head()

loan_ds.shape

# statistical measures
loan_ds.describe()

loan_ds.isnull().sum()

loan_ds = loan_ds.dropna()

loan_ds.isnull().sum()

# Label encoding
loan_ds.replace({"Loan_Status":{'N':0 , 'Y':1}},inplace=True)

loan_ds.head()

# Dependent column values
loan_ds['Dependents'].value_counts()

# replace value of 3+ to 4
loan_ds = loan_ds.replace(to_replace='3+',value=4)

#  dependent values
loan_ds['Dependents'].value_counts()

# Data visualization

# Education and loan status
sns.countplot(x='Education',hue='Loan_Status',data=loan_ds)
# 0 means loan will be rejected and 1 means it will be approved

# marital status & loan Status
sns.countplot(x='Married',hue='Loan_Status',data=loan_ds)
# Logically marital status approval is more bcoz both hus and wife contribute to settling that loan

# convert categorical columns to numerical
loan_ds.replace({'Married':{'No':0,'Yes':1},'Gender':{'Male':1,'Female':0},'Self_Employed':{'No':0,'Yes':1},'Property_Area':{'Rural':0,'Semiurban':1,'Urban':2},
                 'Education':{'Graduate' :1,'Not Graduate':0}} ,inplace=True )

loan_ds.head()

# Separating data and label
X = loan_ds.drop(columns=['Loan_ID','Loan_Status'],axis=1)
# for removing the columns give axis as 1
Y = loan_ds['Loan_Status']

print(X)
print(Y)

#  Data Standardization

scaler = StandardScaler()
scaler.fit(X)
std_data = scaler.transform(X)
#  or .fit_transform

X = std_data
Y = loan_ds['Loan_Status']

print(X)
print(Y)

# Train Test Split

X_train , X_test , Y_train , Y_test = train_test_split(X,Y,test_size=0.1,stratify=Y,random_state=2)
# 90% data is stored in X and 90% is stored in test data.
#  Without stratify we can have high proportion of 0 in train and less proportion of 0 in test..

print(X.shape,X_train.shape , X_test.shape)

# Training the model: Support Vector Machine
classifier = svm.SVC(kernel='linear')

# Training the SVM
classifier.fit(X_train,Y_train)

# Model Evaluation

# Accuracy score on training data
X_train_prediction = classifier.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction,Y_train)

print('Accuracy on training data : ', training_data_accuracy)

#  So from above we can have more accuracy score if we have large dataset

# Accuracy score on testing data
X_test_prediction = classifier.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction,Y_test)
# Accuracy on test data is more important

print('Accuracy on test data : ', test_data_accuracy)

# So we got accuracy of 83% which is really good

# Conclusion: So if u have a data similar to your training data then only your model can predict well ,i.e your model is over depenedent on
# training data i.e our model does not trained well or learnt well. In that case , acccuracy score on training data will be very large and accuracy
# score on test data is very less & this concept is called as overfitting. So avoid over fitting as much as possible.
# Here our model is not overfitted.

# ----- Make a predictive system ----------

from sklearn.preprocessing import StandardScaler

# Input data
input_data =(0, 0, 0, 1, 0, 2900, 0.0, 71.0, 360.0, 1.0, 0)

# Changing the input data to numpy array
np_array = np.asarray(input_data)

# Reshape the array as we are predicting for one instance
np_array_reshaped = np_array.reshape(1, -1)

# Standardized the input data
stdd_data = scaler.transform(np_array_reshaped)
print(stdd_data)

prediction = classifier.predict(np_array_reshaped)
print(prediction)

if(prediction[0]==1):
    print("Loan is approved")
else:
    print("Loan is rejected")

# Saving the trained model

import pickle

filename = 'loan_model.sav'
pickle.dump(classifier,open(filename,'wb'))

# Loading the saved model
loaded_model=pickle.load(open('loan_model.sav','rb'))

from sklearn.preprocessing import StandardScaler

# Input data
input_data =(0, 0, 0, 1, 0, 2900, 0.0, 71.0, 360.0, 1.0, 0)

# Changing the input data to numpy array
np_array = np.asarray(input_data)

# Reshape the array as we are predicting for one instance
np_array_reshaped = np_array.reshape(1, -1)

# Standardized the input data
stdd_data = scaler.transform(np_array_reshaped)
print(stdd_data)

prediction = loaded_model.predict(np_array_reshaped)
print(prediction)

if(prediction[0]==1):
    print("Loan is approved")
else:
    print("Loan is rejected")